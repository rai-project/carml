{
  "manifests": [
    {
      "attributes": {
        "Top1": "54.92",
        "Top5": "78.03",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use AlexNet from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "4abd57ec8863ff3e3e29ecd4ead43d1f",
        "graph_path": "model-symbol.json",
        "weights_checksum": "906234b2a6b14bedac2dcccba8178529",
        "weights_path": "model-0000.params"
      },
      "name": "AlexNet",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "93.0",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "CIFAR10"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use CIFAR_ResNet110_v1 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "CIFAR_ResNet110_v1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "94.3",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "CIFAR10"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use CIFAR_ResNet110_v2 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "CIFAR_ResNet110_v2",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "92.1",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "CIFAR10"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use CIFAR_ResNet20_v1 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "CIFAR_ResNet20_v1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "92.1",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "CIFAR10"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use CIFAR_ResNet20_v2 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "CIFAR_ResNet20_v2",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "93.6",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "CIFAR10"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use CIFAR_ResNet56_v1 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "CIFAR_ResNet56_v1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "93.7",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "CIFAR10"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use CIFAR_ResNet56_v2 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "CIFAR_ResNet56_v2",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "96.3",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "CIFAR10"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use CIFAR_ResNext29_16x64d from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "CIFAR_ResNext29_16x64d",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "CIFAR10"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use CIFAR_ResNext29_32x4d from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "CIFAR_ResNext29_32x4d",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "95.1",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "CIFAR10"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use CIFAR_WideResNet16_10 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "CIFAR_WideResNet16_10",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "95.6",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "CIFAR10"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use CIFAR_WideResNet28_10 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "CIFAR_WideResNet28_10",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "95.9",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "CIFAR10"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use CIFAR_WideResNet40_8 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "CIFAR_WideResNet40_8",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "78.56",
        "Top5": "94.43",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use Darknet53 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "Darknet53",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "74.97",
        "Top5": "92.25",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use DenseNet121 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "DenseNet121",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "77.70",
        "Top5": "93.80",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use DenseNet161 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "DenseNet161",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "76.17",
        "Top5": "93.17",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use DenseNet169 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "DenseNet169",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "77.32",
        "Top5": "93.62",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use DenseNet201 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "DenseNet201",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "IoU threshold": "0.5",
        "kind": "CNN",
        "mAP": "78.3",
        "manifest_author": "Cheng Li",
        "training_dataset": "Pascal VOC"
      },
      "description": "MXNet Object Detection model, which is trained on the Pascal VOC dataset. Use faster_rcnn_resnet50_v1b_voc from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "Faster_RCNN_ResNet50_v1b_VOC",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/detection.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "78.77",
        "Top5": "94.39",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use Inception_v3 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "Inception_v3",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "52.91",
        "Top5": "76.94",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use MobileNet0.25 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "MobileNet_0.25",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "65.20",
        "Top5": "86.34",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use MobileNet0.5 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "MobileNet_0.5",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "70.25",
        "Top5": "89.49",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use MobileNet0.75 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "MobileNet_0.75",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "73.28",
        "Top5": "91.30",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use MobileNet1.0 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "MobileNet_1.0",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "72.85",
        "Top5": "90.99",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use MobileNet1.0_int8 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "MobileNet_1.0_int8",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "51.76",
        "Top5": "74.89",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use MobileNetv2_0.25 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "MobileNet_v2_0.25",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "64.43",
        "Top5": "85.31",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use MobileNetv2_0.5 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "MobileNet_v2_0.5",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "69.36",
        "Top5": "88.50",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use MobileNetv2_0.75 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "MobileNet_v2_0.75",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "72.04",
        "Top5": "90.57",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use MobileNetv2_1.0 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "MobileNet_v2_1.0",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "78.34",
        "Top5": "94.01",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet101_v1 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet101_v1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "79.20",
        "Top5": "94.61",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet101_v1b from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet101_v1b",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "79.60",
        "Top5": "94.75",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet101_v1c from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet101_v1c",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "80.51",
        "Top5": "95.12",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet101_v1d from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet101_v1d",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "78.53",
        "Top5": "94.17",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet101_v2 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet101_v2",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "79.22",
        "Top5": "94.64",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet152_v1 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet152_v1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "79.69",
        "Top5": "94.74",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet152_v1b from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet152_v1b",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "80.01",
        "Top5": "94.96",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet152_v1c from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet152_v1c",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "80.61",
        "Top5": "95.34",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet152_v1d from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet152_v1d",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "79.21",
        "Top5": "94.31",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet152_v2 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet152_v2",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "70.93",
        "Top5": "89.92",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet18_v1 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "b10b9148d06b184a8ca4af3b06fe403e",
        "graph_path": "model-symbol.json",
        "weights_checksum": "fa9e658e88d4259b307a50b36a080bdc",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet18_v1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "70.94",
        "Top5": "89.83",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet18_v1b from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet18_v1b",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "71.00",
        "Top5": "89.92",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet18_v2 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet18_v2",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "74.37",
        "Top5": "91.87",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet34_v1 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet34_v1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "74.65",
        "Top5": "92.08",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet34_v1b from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet34_v1b",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "74.40",
        "Top5": "92.08",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet34_v2 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet34_v2",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "77.36",
        "Top5": "93.57",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet50_v1 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet50_v1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "76.86",
        "Top5": "93.46",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet50_v1_int8 from GluonCV model zoo. ResNet50_v1_int8 is a quantized model for ResNet50_v1.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet50_v1_int8",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "77.67",
        "Top5": "93.82",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet50_v1b from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet50_v1b",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "77.36",
        "Top5": "93.59",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet50_v1b_gn from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet50_v1b_gn",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "78.03",
        "Top5": "94.09",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet50_v1c from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet50_v1c",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "79.15",
        "Top5": "94.58",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet50_v1d from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet50_v1d",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "77.11",
        "Top5": "93.43",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNet50_v2 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNet50_v2",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "80.37",
        "Top5": "95.06",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNext101_32x4d from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNext101_32x4d",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "80.69",
        "Top5": "95.17",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNext101_64x4d_v1 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNext101_64x4d_v1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "79.32",
        "Top5": "94.53",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use ResNext50_32x4d from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "ResNext50_32x4d",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "80.91",
        "Top5": "95.39",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use SE_ResNext101_32x4d from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "SE_ResNext101_32x4d",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "81.01",
        "Top5": "95.32",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use SE_ResNext101_64x4d from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "SE_ResNext101_64x4d",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "79.95",
        "Top5": "94.93",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use SE_ResNext50_32x4d from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "SE_ResNext50_32x4d",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "81.26",
        "Top5": "95.51",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use SENet_154 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "SENet_154",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "56.11",
        "Top5": "79.09",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use SqueezeNet_v1.0 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "ddf014f8b42e26d8d60f9cc5803f8cf3",
        "graph_path": "model-symbol.json",
        "weights_checksum": "8cf396e2ec24691020fae29ffc98b88a",
        "weights_path": "model-0000.params"
      },
      "name": "SqueezeNet_v1.0",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "54.96",
        "Top5": "78.17",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use SqueezeNet_v1.1 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "4540936dae06bf9304838e3df88c24e8",
        "graph_path": "model-symbol.json",
        "weights_checksum": "1351611541c24b57015aee487f4b7d70",
        "weights_path": "model-0000.params"
      },
      "name": "SqueezeNet_v1.1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Box AP": "25.1/42.9/25.8",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "COCO"
      },
      "description": "MXNet Object Detection model, which is trained on the COCO dataset. Use ssd_300_vgg16_atrous_coco from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "SSD_300_VGG16_Atrous_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/detection.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "IoU threshold": "0.5",
        "kind": "CNN",
        "mAP": "77.6",
        "manifest_author": "Cheng Li",
        "training_dataset": "Pascal VOC"
      },
      "description": "MXNet Object Detection model, which is trained on the Pascal VOC dataset. Use ssd_300_vgg16_atrous_voc from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "SSD_300_VGG16_Atrous_VOC",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/detection.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Box AP": "21.7/39.2/21.3",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "COCO"
      },
      "description": "MXNet Object Detection model, which is trained on the Pascal VOC dataset. Use ssd_512_resnet101_v2_coco from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "SSD_512_MobileNet_1.0_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/detection.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "IoU threshold": "0.5",
        "kind": "CNN",
        "mAP": "75.4",
        "manifest_author": "Cheng Li",
        "training_dataset": "Pascal VOC"
      },
      "description": "MXNet Object Detection model, which is trained on the Pascal VOC dataset. Use ssd_512_mobilenet1.0_voc from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "SSD_512_MobileNet_1.0_VOC",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/detection.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "Pascal VOC"
      },
      "description": "MXNet Object Detection model, which is trained on the Pascal VOC dataset. Use ssd_512_resnet101_v2_voc from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "SSD_512_ResNet101_v2_VOC",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/detection.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Box AP": "30.6/50.0/32.2",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "COCO"
      },
      "description": "MXNet Object Detection model, which is trained on the COCO dataset. Use ssd_512_resnet50_v1_coco from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "SSD_512_ResNet50_v1_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/detection.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "IoU threshold": "0.5",
        "kind": "CNN",
        "mAP": "80.1",
        "manifest_author": "Cheng Li",
        "training_dataset": "Pascal VOC"
      },
      "description": "MXNet Object Detection model, which is trained on the Pascal VOC dataset. Use ssd_512_resnet50_v1_voc from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "SSD_512_ResNet50_v1_VOC",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/detection.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Box AP": "28.9/47.9/30.6",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "COCO"
      },
      "description": "MXNet Object Detection model, which is trained on the COCO dataset. Use ssd_512_vgg16_atrous_coco from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "SSD_512_VGG16_Atrous_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/detection.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "IoU threshold": "0.5",
        "kind": "CNN",
        "mAP": "79.2",
        "manifest_author": "Cheng Li",
        "training_dataset": "Pascal VOC"
      },
      "description": "MXNet Object Detection model, which is trained on the Pascal VOC dataset. Use ssd_512_vgg16_atrous_voc from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "SSD_512_VGG16_Atrous_VOC",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/detection.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "66.62",
        "Top5": "87.34",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use VGG11 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "VGG11",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "68.59",
        "Top5": "88.72",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use VGG11 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "VGG11_bn",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "67.74",
        "Top5": "88.11",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use VGG13 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "VGG13",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "68.84",
        "Top5": "88.11",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use VGG13_bn from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "VGG13_bn",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "73.23",
        "Top5": "91.31",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use VGG16 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "VGG16",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "73.10",
        "Top5": "91.76",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use VGG16_bn from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "VGG16_bn",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "74.11",
        "Top5": "91.35",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use VGG19 from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "VGG19",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "Top1": "74.33",
        "Top5": "91.85",
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "MXNet Image Classification model, which is trained on the ImageNet dataset. Use VGG19_bn from GluonCV model zoo.\n",
      "framework": {
        "name": "MXNet",
        "version": "1.7.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_path": "model-symbol.json",
        "weights_path": "model-0000.params"
      },
      "name": "VGG19_bn",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://gluon-cv.mxnet.io/model_zoo/classification.html",
        "https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/verify_pretrained.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the Resnet publication originally written in Caffe. The pre-trained model expects input in mini-batches of 3-channel BGR images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 255] and then normalized using mean = [102.9801, 115.9465, 122.7717] and std = [1, 1, 1]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "9f029061ed3b841cb8bf35faa72ba9ef",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/cafferesnet101-imagenet.onnx"
      },
      "name": "Caffe_ResNet_101",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/cafferesnet.py",
        "https://github.com/KaimingHe/deep-residual-networks"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the Dual Path Networks publication. The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [124 / 255, 117 / 255, 104 / 255] and std = [1 / (.0167 * 255), 1 / (.0167 * 255), 1 / (.0167 * 255)]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "116b2057a202acce4f154afb8f2efbb5",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/dpn107-imagenet.onnx"
      },
      "name": "DPN_107",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/dpn.py",
        "https://github.com/cypw/DPNs",
        "https://github.com/oyam/pytorch-DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the Dual Path Networks publication. The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [124 / 255, 117 / 255, 104 / 255] and std = [1 / (.0167 * 255), 1 / (.0167 * 255), 1 / (.0167 * 255)]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "ce7898dd8858a699e1a22cf784e9d5e2",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/dpn131-imagenet.onnx"
      },
      "name": "DPN_131",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/dpn.py",
        "https://github.com/cypw/DPNs",
        "https://github.com/oyam/pytorch-DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the Dual Path Networks publication. The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [124 / 255, 117 / 255, 104 / 255] and std = [1 / (.0167 * 255), 1 / (.0167 * 255), 1 / (.0167 * 255)]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "296084b9bb1c1333c92b6b4763903d3a",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/dpn68-imagenet.onnx"
      },
      "name": "DPN_68_v1.0",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/dpn.py",
        "https://github.com/cypw/DPNs",
        "https://github.com/oyam/pytorch-DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the Dual Path Networks publication. The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [124 / 255, 117 / 255, 104 / 255] and std = [1 / (.0167 * 255), 1 / (.0167 * 255), 1 / (.0167 * 255)]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "627f2708c3e3c87c57a37399e7c6ae0f",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/dpn68b-imagenet.onnx"
      },
      "name": "DPN_68_v2.0",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/dpn.py",
        "https://github.com/cypw/DPNs",
        "https://github.com/oyam/pytorch-DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the Dual Path Networks publication. The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [124 / 255, 117 / 255, 104 / 255] and std = [1 / (.0167 * 255), 1 / (.0167 * 255), 1 / (.0167 * 255)]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "8775db63e9a1562465f689521df9d072",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/dpn92-imagenet.onnx"
      },
      "name": "DPN_92",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/dpn.py",
        "https://github.com/cypw/DPNs",
        "https://github.com/oyam/pytorch-DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the Dual Path Networks publication. The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [124 / 255, 117 / 255, 104 / 255] and std = [1 / (.0167 * 255), 1 / (.0167 * 255), 1 / (.0167 * 255)]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "4fe9bf94ebaa91e33b3131e1ae94ceb6",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/dpn98-imagenet.onnx"
      },
      "name": "DPN_98",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/dpn.py",
        "https://github.com/cypw/DPNs",
        "https://github.com/oyam/pytorch-DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 299. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.5, 0.5, 0.5] and std = [0.5, 0.5, 0.5]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "d312c2726d398358f73440f20eb0c87f",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/inceptionresnetv2-imagenet.onnx"
      },
      "name": "Inception_ResNet_v2.0",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/inceptionresnetv2.py",
        "https://arxiv.org/abs/1602.07261"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 299. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.5, 0.5, 0.5] and std = [0.5, 0.5, 0.5]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "31709e7318796f5fc984cedd028100a4",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/inceptionv3-imagenet.onnx"
      },
      "name": "Inception_v3.0",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/torchvision_models.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "MLPerf_Mobilenet_v1_1.0_224. Use mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224.tgz from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "5173c0ccbae0d4fe2acd514e8b847f0d",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/mobilenet_v1_1.0_224.onnx"
      },
      "name": "MLPerf_Mobilenet_v1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/mlperf/inference/tree/master/v0.5/classification_and_detection",
        "https://github.com/mlperf/training/tree/master/image_classification",
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "MLPerf_ResNet50_v1.5.\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "a638cf028b5870da29e09ccc2f7182e7",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/resnet50_v1.onnx"
      },
      "name": "MLPerf_ResNet50_v1.5",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1512.03385.pdf",
        "https://github.com/mlperf/inference/tree/master/v0.5/classification_and_detection",
        "https://github.com/mlperf/training/tree/master/image_classification"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "COCO 2017"
      },
      "description": "MLPerf_SSD_ResNet34_1200x1200.\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "b70fc6c72bc9349981f3b1258f31bc87",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/resnet34-ssd1200.onnx"
      },
      "name": "MLPerf_SSD_ResNet34_1200x1200",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/mlperf/inference/tree/master/v0.5/classification_and_detection",
        "https://github.com/mlperf/inference/tree/master/others/cloud/single_stage_detector/tensorflow"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the MobileNet publication. This model expects mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 300. The images have to be loaded in to a range of [0, 255] and then normalized using mean = [127, 127, 127] and std = [128, 128, 128]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "ac33959f371eaed1be866499af3b2894",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/mb2-ssd-lite.onnx"
      },
      "name": "MobileNet_SSD_Lite_v2.0",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/qfgaohao/pytorch-ssd/blob/master/vision/nn/mobilenet.py",
        "https://github.com/qfgaohao/pytorch-ssd/blob/master/vision/ssd/mobilenet_v2_ssd_lite.py",
        "https://storage.googleapis.com/models-hao/mb2-ssd-lite-mp-0_686.pth",
        "https://arxiv.org/pdf/1704.04861.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the MobileNet publication. This model expects mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 300. The images have to be loaded in to a range of [0, 255] and then normalized using mean = [127, 127, 127] and std = [128, 128, 128]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "16dff9e111042f981b2c054c6f74f85f",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/mb1-ssd.onnx"
      },
      "name": "MobileNet_SSD_v1.0",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/qfgaohao/pytorch-ssd/blob/master/vision/nn/mobilenet.py",
        "https://github.com/qfgaohao/pytorch-ssd/blob/master/vision/ssd/mobilenetv1_ssd.py",
        "https://storage.googleapis.com/models-hao/mobilenet-v1-ssd-mp-0_675.pth",
        "https://arxiv.org/pdf/1704.04861.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 331. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.5, 0.5, 0.5] and std = [0.5, 0.5, 0.5]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "7e96b0856877acb50471fc2d1f0e23b4",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/nasnetalarge-imagenet.onnx"
      },
      "name": "NasNet_A_Large",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/nasnet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.5, 0.5, 0.5] and std = [0.5, 0.5, 0.5]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "81629351ceba9ced67754312d7dae47d",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/nasnetamobile-imagenet.onnx"
      },
      "name": "NasNet_A_Mobile",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/nasnet_mobile.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "COCO 2017"
      },
      "description": "This model is a real-time neural network for object instance segmentation that detects 80 different classes. This model only allows batchsize = 1 currently.\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "ba1088fe5c4a9182075f3ce136037968",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/onnxvision_mask_rcnn_r_50_fpn.onnx"
      },
      "name": "OnnxVision_Mask_RCNN_R_50_FPN",
      "output": {
        "description": "the output instance segment",
        "type": "instancesegment"
      },
      "reference": [
        "https://github.com/onnx/models/tree/master/vision/object_detection_segmentation/mask-rcnn"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "COCO 2017"
      },
      "description": "This model is the Single Stage Detector in onnx vision All pre-trained models expect input images normalized in the same way, i.e. 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 1200. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225] This model only allows batchsize = 1 currently.\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "c87961aa865330bbb6eab9687ae2496c",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/onnxvision_ssd.onnx"
      },
      "name": "OnnxVision_SSD",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/onnx/models/tree/master/vision/object_detection_segmentation/ssd"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 331. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.5, 0.5, 0.5] and std = [0.5, 0.5, 0.5]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "c652ab5d706bcf3bed98fc8d50b877d1",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/pnasnet5large-imagenet.onnx"
      },
      "name": "PNasNet_5_Large",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/pnasnet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 331. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "320070d5e8cde63ae577daf3d9dceb1c",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/polynet-imagenet.onnx"
      },
      "name": "PolyNet",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/polynet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "d1685cbf522dc6f9d22fb92332e9b698",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/resnext101_32x4d-imagenet.onnx"
      },
      "name": "ResNext_101_32x4D",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/resnext.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "0e3d0b2b3896d8edb1f394cf0c7bf2b2",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/resnext101_64x4d-imagenet.onnx"
      },
      "name": "ResNext_101_64x4D",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/resnext.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "1016faf78402719b45f86b4e7d2dc105",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/se_resnet101-imagenet.onnx"
      },
      "name": "SE_ResNet_101",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "e6526ad541c289e9de9d87546ce1fd8b",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/se_resnet152-imagenet.onnx"
      },
      "name": "SE_ResNet_152",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "132b3cf809c7f7ddfbc69a2a480e14b5",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/se_resnet50-imagenet.onnx"
      },
      "name": "SE_ResNet_50",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "58cf591e3e457cb4dfc98d2facc7f732",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/se_resnext101_32x4d-imagenet.onnx"
      },
      "name": "SE_ResNext_101_32x4D",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "fd3adc82ce4bbf683ddadfee846e2f65",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/se_resnext50_32x4d-imagenet.onnx"
      },
      "name": "SE_ResNext_50_32x4D",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "c652ab5d706bcf3bed98fc8d50b877d1",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/senet154-imagenet.onnx"
      },
      "name": "SENet_154",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "VOC"
      },
      "description": "This model is a replication of the model described in the SRGAN publication.\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "4527947ddf80f3da2bc9a216b6fb813b",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/srgan.onnx"
      },
      "name": "SRGAN",
      "output": {
        "description": "the output image",
        "type": "image"
      },
      "reference": [
        "https://github.com/leftthomas/SRGAN",
        "https://arxiv.org/pdf/1609.04802.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the AlexNet publication. The model is generated using torch.onnx based on the torchvision model All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "1fa49494d4b1c24a0ee5e871930343de",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_alexnet.onnx"
      },
      "name": "TorchVision_AlexNet",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/abs/1404.5997",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py",
        "https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.alexnet"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "COCO 2017"
      },
      "description": "The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "7ab5a923c5b4f673fd54ec863a3da1c5",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_deeplabv3_resnet101.onnx"
      },
      "name": "TorchVision_DeepLabv3_Resnet101",
      "output": {
        "description": "the output semantic segment",
        "type": "semanticsegment"
      },
      "reference": [
        "https://arxiv.org/pdf/1706.05587.pdf",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/segmentation/deeplabv3.py",
        "https://pytorch.org/docs/stable/torchvision/models.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the DenseNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "811de9078d1c8a84b6764362fe77d7f0",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_densenet121.onnx"
      },
      "name": "TorchVision_DenseNet_121",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1608.06993.pdf",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the DenseNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "e4cc70e582678ec4e8ae358c53394f0f",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_densenet161.onnx"
      },
      "name": "TorchVision_DenseNet_161",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1608.06993.pdf",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the DenseNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "66b0c9eb4f98360dc33f31d882fa0dce",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_densenet169.onnx"
      },
      "name": "TorchVision_DenseNet_169",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1608.06993.pdf",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the DenseNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "52cb6ceff5c2e931de1721f855a2b239",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_densenet201.onnx"
      },
      "name": "TorchVision_DenseNet_201",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1608.06993.pdf",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "COCO 2017"
      },
      "description": "The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "4b24503a089adba2d10b296adf760c87",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_fcn_resnet101.onnx"
      },
      "name": "TorchVision_FCN_Resnet101",
      "output": {
        "description": "the output semantic segment",
        "type": "semanticsegment"
      },
      "reference": [
        "https://arxiv.org/pdf/1411.4038.pdf",
        "https://github.com/pytorch/vision/blob/v0.4.0/torchvision/models/segmentation/fcn.py",
        "https://pytorch.org/docs/stable/torchvision/models.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the ResNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "fa2b1000e97cd16bad07a502f58ece5c",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_resnet101.onnx"
      },
      "name": "TorchVision_ResNet_101",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1512.03385.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the ResNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "4b750e31e1b3973d76816fc0a5919cb5",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_resnet152.onnx"
      },
      "name": "TorchVision_ResNet_152",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1512.03385.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the ResNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "e3faa3710ee19e81c8c1b07e04e2ed7f",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_resnet18.onnx"
      },
      "name": "TorchVision_ResNet_18",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1512.03385.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the ResNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "3395fe6af20cd222d92a6989454973d4",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_resnet34.onnx"
      },
      "name": "TorchVision_ResNet_34",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1512.03385.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the ResNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "5b48facd960761ac090c5977957fc416",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_resnet50.onnx"
      },
      "name": "TorchVision_ResNet_50",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1512.03385.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the ResNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "b1d4e11c2d31dad1e17770b49a3d6a30",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_squeezenet1_0.onnx"
      },
      "name": "TorchVision_SqueezeNet_v1.0",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1602.07360.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the ResNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "c124ba759a39a627e0923be8e83cd4ab",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_squeezenet1_1.onnx"
      },
      "name": "TorchVision_SqueezeNet_v1.1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1602.07360.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "44a6ad9b498c903d08216281872c38cf",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_vgg11.onnx"
      },
      "name": "TorchVision_VGG_11",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "7a2d61adf621185e5c2605c5091f974c",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_vgg11_bn.onnx"
      },
      "name": "TorchVision_VGG_11_BN",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "f8dc46560866f5267c8b14ab93ae096e",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_vgg13.onnx"
      },
      "name": "TorchVision_VGG_13",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "5bc05c3b0b35e0d082c199cf7d511acd",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_vgg13_bn.onnx"
      },
      "name": "TorchVision_VGG_13_BN",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "e104cbcef280629aa230c556821dc4f6",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_vgg16.onnx"
      },
      "name": "TorchVision_VGG_16",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "be4e4246df2293e3102dd2d742b0f6c3",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_vgg16_bn.onnx"
      },
      "name": "TorchVision_VGG_16_BN",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "5082bb6a5720311febce81de4aa355c7",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_vgg19.onnx"
      },
      "name": "TorchVision_VGG_19",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "7a5a8c9da5d248473e9e81e7238dd169",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/torchvision_vgg19_bn.onnx"
      },
      "name": "TorchVision_VGG_19_BN",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 229. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.5, 0.5, 0.5] and std = [0.5, 0.5, 0.5]\n",
      "framework": {
        "name": "Onnxruntime",
        "version": "1.6.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "608d5b80fd6119e5ec19b3dd3d217167",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/onnxruntime/xception-imagenet.onnx"
      },
      "name": "Xception",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the Resnet publication originally written in Caffe. The pre-trained model expects input in mini-batches of 3-channel BGR images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 255] and then normalized using mean = [102.9801, 115.9465, 122.7717] and std = [1, 1, 1]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "656bf13acb441b4427377acec10bb3e0",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/cafferesnet101-imagenet.pt"
      },
      "name": "Caffe_ResNet_101",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/cafferesnet.py",
        "https://github.com/KaimingHe/deep-residual-networks"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the Dual Path Networks publication. The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [124 / 255, 117 / 255, 104 / 255] and std = [1 / (.0167 * 255), 1 / (.0167 * 255), 1 / (.0167 * 255)]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "d9c9b48f61a035ea2fa05c1fee44cc72",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/dpn107-imagenet.pt"
      },
      "name": "DPN_107",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/dpn.py",
        "https://github.com/cypw/DPNs",
        "https://github.com/oyam/pytorch-DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the Dual Path Networks publication. The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [124 / 255, 117 / 255, 104 / 255] and std = [1 / (.0167 * 255), 1 / (.0167 * 255), 1 / (.0167 * 255)]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "f842d8fdbfa37ec5a1c931815d469585",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/dpn131-imagenet.pt"
      },
      "name": "DPN_131",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/dpn.py",
        "https://github.com/cypw/DPNs",
        "https://github.com/oyam/pytorch-DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the Dual Path Networks publication. The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [124 / 255, 117 / 255, 104 / 255] and std = [1 / (.0167 * 255), 1 / (.0167 * 255), 1 / (.0167 * 255)]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "6287a2d68054975d4d3c82c71742eb21",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/dpn68-imagenet.pt"
      },
      "name": "DPN_68_v1.0",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/dpn.py",
        "https://github.com/cypw/DPNs",
        "https://github.com/oyam/pytorch-DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the Dual Path Networks publication. The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [124 / 255, 117 / 255, 104 / 255] and std = [1 / (.0167 * 255), 1 / (.0167 * 255), 1 / (.0167 * 255)]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "91d59a7c160c162748dba89b35690bd7",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/dpn68b-imagenet.pt"
      },
      "name": "DPN_68_v2.0",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/dpn.py",
        "https://github.com/cypw/DPNs",
        "https://github.com/oyam/pytorch-DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the Dual Path Networks publication. The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [124 / 255, 117 / 255, 104 / 255] and std = [1 / (.0167 * 255), 1 / (.0167 * 255), 1 / (.0167 * 255)]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "58513bd1ae35f120f305eed840734ad1",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/dpn92-imagenet.pt"
      },
      "name": "DPN_92",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/dpn.py",
        "https://github.com/cypw/DPNs",
        "https://github.com/oyam/pytorch-DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the Dual Path Networks publication. The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [124 / 255, 117 / 255, 104 / 255] and std = [1 / (.0167 * 255), 1 / (.0167 * 255), 1 / (.0167 * 255)]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "6806745213fcd7bf0eec71bfe69d88df",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/dpn98-imagenet.pt"
      },
      "name": "DPN_98",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/dpn.py",
        "https://github.com/cypw/DPNs",
        "https://github.com/oyam/pytorch-DPNs"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 299. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.5, 0.5, 0.5] and std = [0.5, 0.5, 0.5]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "5643b8cdce7ea7aed18a7e5f3960cb3b",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/inceptionresnetv2-imagenet.pt"
      },
      "name": "Inception_ResNet_v2.0",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/inceptionresnetv2.py",
        "https://arxiv.org/abs/1602.07261"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 299. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.5, 0.5, 0.5] and std = [0.5, 0.5, 0.5]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "f0372bbb562d22e64815ad84387776ce",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/inceptionv3-imagenet.pt"
      },
      "name": "Inception_v3.0",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/torchvision_models.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the MobileNet publication. This model expects mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 300. The images have to be loaded in to a range of [0, 255] and then normalized using mean = [127, 127, 127] and std = [128, 128, 128]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "31107eb8bc8bdc40bd663f8bddce0ebb",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/mb2-ssd-lite.pt"
      },
      "name": "MobileNet_SSD_Lite_v2.0",
      "output": {
        "description": "the output label",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/qfgaohao/pytorch-ssd/blob/master/vision/nn/mobilenet.py",
        "https://github.com/qfgaohao/pytorch-ssd/blob/master/vision/ssd/mobilenet_v2_ssd_lite.py",
        "https://storage.googleapis.com/models-hao/mb2-ssd-lite-mp-0_686.pth",
        "https://arxiv.org/pdf/1704.04861.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the MobileNet publication. This model expects mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 300. The images have to be loaded in to a range of [0, 255] and then normalized using mean = [127, 127, 127] and std = [128, 128, 128]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "83ce8910a6a502456a5033c3509fc6d2",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/mb1-ssd.pt"
      },
      "name": "MobileNet_SSD_v1.0",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/qfgaohao/pytorch-ssd/blob/master/vision/nn/mobilenet.py",
        "https://github.com/qfgaohao/pytorch-ssd/blob/master/vision/ssd/mobilenetv1_ssd.py",
        "https://storage.googleapis.com/models-hao/mobilenet-v1-ssd-mp-0_675.pth",
        "https://arxiv.org/pdf/1704.04861.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 331. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.5, 0.5, 0.5] and std = [0.5, 0.5, 0.5]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "b82361909cae3a7da7cd0bfc18125eab",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/nasnetalarge-imagenet.pt"
      },
      "name": "NasNet_A_Large",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/nasnet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.5, 0.5, 0.5] and std = [0.5, 0.5, 0.5]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "0a26aed51897cad244f2aa542e7c1e3c",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/nasnetamobile-imagenet.pt"
      },
      "name": "NasNet_A_Mobile",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/nasnet_mobile.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 331. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.5, 0.5, 0.5] and std = [0.5, 0.5, 0.5]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "07bc0bc89e1127c153de8cb8f77d0972",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/pnasnet5large-imagenet.pt"
      },
      "name": "PNasNet_5_Large",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/pnasnet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 331. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "d17f09bddd7f166895c0bfdb293c3622",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/polynet-imagenet.pt"
      },
      "name": "PolyNet",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/polynet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "883ebb742cb6b0b93f3c5b2de65ecc5a",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/resnext101_32x4d-imagenet.pt"
      },
      "name": "ResNext101_32x4D",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/resnext.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "036f4013faa486c53413657148c96281",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/resnext101_64x4d-imagenet.pt"
      },
      "name": "ResNext101_64x4D",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/resnext.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "941ac83601681306e652d76bec5966f8",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/se_resnet101-imagenet.pt"
      },
      "name": "SE_ResNet_101",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "0f882b314c663ee64edaf201beda5175",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/se_resnet152-imagenet.pt"
      },
      "name": "SE_ResNet_152",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "59d9439414448bc644238eb27ce94e7a",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/se_resnet50-imagenet.pt"
      },
      "name": "SE_ResNet_50",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "353df39873bd70397591f5e68dfff8cb",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/se_resnext101_32x4d-imagenet.pt"
      },
      "name": "SE_ResNext_101_32x4D",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "34dc3df9301edc3c4583300587a67ed9",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/se_resnext50_32x4d-imagenet.pt"
      },
      "name": "SE_ResNext_50_32x4D",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "989c3a72ebaf30596b3b8bf6488180ac",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/senet154-imagenet.pt"
      },
      "name": "SENet_154",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results",
        "https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "VOC"
      },
      "description": "This model is a replication of the model described in the SRGAN publication.\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "2ea83f6e74420f60902e074fb0e893df",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/srgan_netG_epoch_4_100.pt"
      },
      "name": "SRGAN_v1.0",
      "output": {
        "description": "the output image",
        "type": "image"
      },
      "reference": [
        "https://github.com/leftthomas/SRGAN",
        "https://arxiv.org/pdf/1609.04802.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the AlexNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "50a8a0f7cb9c3caa8f50a89b59a820a9",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/alexnet.pt"
      },
      "name": "TorchVision_AlexNet",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/abs/1404.5997",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "COCO 2017"
      },
      "description": "The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "8adc41a27060bfadfe6fcde02dff59a0",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/deeplabv3_resnet101.pt"
      },
      "name": "TorchVision_DeepLabv3_Resnet101",
      "output": {
        "description": "the output semantic segment",
        "type": "semanticsegment"
      },
      "reference": [
        "https://arxiv.org/pdf/1706.05587.pdf",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/segmentation/deeplabv3.py",
        "https://pytorch.org/docs/stable/torchvision/models.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the DenseNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "bae13d1824d4b2d8bb1f99bf83e0bbd4",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/densenet121.pt"
      },
      "name": "TorchVision_DenseNet_121",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1608.06993.pdf",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the DenseNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "97ce2d83f49bede95e16bede0f9905f5",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/densenet161.pt"
      },
      "name": "TorchVision_DenseNet_161",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1608.06993.pdf",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the DenseNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "b6a86beb5d9932890a3e06a46f953b3a",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/densenet169.pt"
      },
      "name": "TorchVision_DenseNet_169",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1608.06993.pdf",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the DenseNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "012c4edd769bc9444d8037fb5f789832",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/densenet201.pt"
      },
      "name": "TorchVision_DenseNet_201",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1608.06993.pdf",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Yen-Hsiang Chang",
        "training_dataset": "COCO 2017"
      },
      "description": "The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "5e0b3d0a594d561a566e792a79da93c9",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/fcn_resnet101.pt"
      },
      "name": "TorchVision_Fcn_Resnet101",
      "output": {
        "description": "the output semantic segment",
        "type": "semanticsegment"
      },
      "reference": [
        "https://arxiv.org/pdf/1411.4038.pdf",
        "https://github.com/pytorch/vision/blob/v0.4.0/torchvision/models/segmentation/fcn.py",
        "https://pytorch.org/docs/stable/torchvision/models.html"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the ResNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "c4b6541bbcea329ccd7fd5bbb72733df",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/resnet101.pt"
      },
      "name": "TorchVision_ResNet_101",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1512.03385.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the ResNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "2de9bc1451c96ba566a275c9082c37d8",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/resnet152.pt"
      },
      "name": "TorchVision_ResNet_152",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1512.03385.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the ResNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "1664b51cd7e6595d545e9cfad38987da",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/resnet18.pt"
      },
      "name": "TorchVision_ResNet_18",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1512.03385.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the ResNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "0997634a4d13c3a809057e79c563dc5e",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/resnet34.pt"
      },
      "name": "TorchVision_ResNet_34",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1512.03385.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the ResNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "0aea66ce0fe0e27497ff1b82c0a2f925",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/resnet50.pt"
      },
      "name": "TorchVision_ResNet_50",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1512.03385.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the ResNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "0cf61de39de571df51373b5ea52a8352",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/squeezenet1_0.pt"
      },
      "name": "TorchVision_SqueezeNet_v1.0",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1602.07360.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the ResNet publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "7cc2886d7d7b88c62fa34cfebd7d55ff",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/squeezenet1_1.pt"
      },
      "name": "TorchVision_SqueezeNet_v1.1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1602.07360.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "07523ccdc0d8cfed64c79ef0a0acd8ba",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/vgg11.pt"
      },
      "name": "TorchVision_VGG_11",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "105a37baa5cf89ab8d0ca866dfa5b66e",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/vgg11_bn.pt"
      },
      "name": "TorchVision_VGG_11_BN",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "f84b60fdb920f5917bcdaa607d147bdd",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/vgg13.pt"
      },
      "name": "TorchVision_VGG_13",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "af5f5a23877ceab4783fc0580092c3f7",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/vgg13_bn.pt"
      },
      "name": "TorchVision_VGG_13_BN",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "09bf5984508443e87aa7e680b9e52d76",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/vgg16.pt"
      },
      "name": "TorchVision_VGG_16",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "ebe2f07740d819e74bdf3ffcb68780fe",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/vgg16_bn.pt"
      },
      "name": "TorchVision_VGG_16_BN",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "0cfdb98cb02ce5a727ea6c688127122d",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/vgg19.pt"
      },
      "name": "TorchVision_VGG_19",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the VGG publication. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "7438737efb01d6b1c410cae4a2efd14d",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/vgg19_bn.pt"
      },
      "name": "TorchVision_VGG_19_BN",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py",
        "https://pytorch.org/docs/stable/torchvision/index.html",
        "https://arxiv.org/pdf/1409.1556.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "The pre-trained model expects input in mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be 229. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.5, 0.5, 0.5] and std = [0.5, 0.5, 0.5]\n",
      "framework": {
        "name": "PyTorch",
        "version": "1.5.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "c1fd34618d69e408a7517467c89d0147",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/pytorch/xception-imagenet.pt"
      },
      "name": "Xception",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/Cadene/pretrained-models.pytorch#reproducing-results"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "73.29",
        "top5": "91.45",
        "training_dataset": "ImageNet"
      },
      "description": "AI Matrix Densenet121.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "2b8c19a4dbe84afecf22559d234f48c8",
        "graph_path": "Densenet121-NHWC.pb"
      },
      "name": "AI_Matrix_Densenet121",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/alibaba/ai-matrix/tree/master/macro_benchmark/CNN_Tensorflow",
        "https://arxiv.org/pdf/1608.06993.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CTR",
        "manifest_author": "Cheng Li",
        "top1": "",
        "top5": "",
        "training_dataset": ""
      },
      "description": "AI Matrix DIEN.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "mid_his_ph",
          "type": "raw"
        },
        {
          "description": "cat_his_ph",
          "type": "raw"
        },
        {
          "description": "uid_batch_ph",
          "type": "raw"
        },
        {
          "description": "mid_batch_ph",
          "type": "raw"
        },
        {
          "description": "cat_batch_ph",
          "type": "raw"
        },
        {
          "description": "mask",
          "type": "raw"
        },
        {
          "description": "seq_len_ph",
          "type": "raw"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "7be74fc1c1eb4ace05aed4547dfd7e89",
        "graph_path": "DIEN.pb"
      },
      "name": "AI_Matrix_DIEN",
      "output": {
        "description": "the output",
        "type": "raw"
      },
      "reference": [
        "https://github.com/alibaba/ai-matrix/tree/master/macro_benchmark/DIEN",
        "https://arxiv.org/pdf/1809.03672.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "70.01",
        "top5": "89.29",
        "training_dataset": "ImageNet"
      },
      "description": "AI Matrix GoogleNet.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "6efe5eb229bfd1d1e28cd2c42f927167",
        "graph_path": "GoogleNet-NCWH.pb"
      },
      "name": "AI_Matrix_GoogleNet",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/alibaba/ai-matrix/tree/master/macro_benchmark/CNN_Tensorflow",
        "https://arxiv.org/abs/1409.4842"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "75.93",
        "top5": "93.00",
        "training_dataset": "ImageNet"
      },
      "description": "AI Matrix ResNet152.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "f3c0052de27ebfee8795d01755667f23",
        "graph_path": "Resnet152-NCWH.pb"
      },
      "name": "AI_Matrix_ResNet152",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/alibaba/ai-matrix/tree/master/macro_benchmark/CNN_Tensorflow",
        "https://arxiv.org/abs/1409.4842"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "74.38",
        "top5": "91.97",
        "training_dataset": "ImageNet"
      },
      "description": "AI Matrix ResNet50.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "8c17158225e976abc0c06100e095c113",
        "graph_path": "Resnet50-NCWH.pb"
      },
      "name": "AI_Matrix_ResNet50",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/alibaba/ai-matrix/tree/master/macro_benchmark/CNN_Tensorflow",
        "https://arxiv.org/abs/1409.4842"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "abduld",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the AlexNet publication. Differences: not training with the relighting data-augmentation; initializing non-zero biases to 0.1 instead of 1 (found necessary for training, as initialization to 1 gave flat loss). The bundled model is the iteration 360,000 snapshot. The best validation performance during training was iteration 358,000 with validation accuracy 57.258% and loss 1.83948. This model obtains a top-1 accuracy 57.1% and a top-5 accuracy 80.2% on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy.) This model was trained by Evan Shelhamer @shelhamer\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "6d23f40191c1dcac71285f41a85abd8e",
        "graph_path": "bvlc_alexnet_1.0/frozen_model.pb"
      },
      "name": "BVLC_AlexNet_Caffe",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/BVLC/Caffe/tree/master/models/bvlc_alexnet",
        "https://github.com/BVLC/Caffe/wiki/Models-accuracy-on-ImageNet-2012-val",
        "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "This model is a replication of the model described in the GoogleNet publication. We would like to thank Christian Szegedy for all his help in the replication of GoogleNet model. Differences: not training with the relighting data-augmentation; not training with the scale or aspect-ratio data-augmentation; uses \"xavier\" to initialize the weights instead of \"gaussian\"; quick_solver.prototxt uses a different learning rate decay policy than the original solver.prototxt, that allows a much faster training (60 epochs vs 250 epochs); The bundled model is the iteration 2,400,000 snapshot (60 epochs) using quick_solver.prototxt This bundled model obtains a top-1 accuracy 68.7% (31.3% error) and a top-5 accuracy 88.9% (11.1% error) on the validation set, using just the center crop. (Using the average of 10 crops, (4 + 1 center) * 2 mirror, should obtain a bit higher accuracy.) Timings for bvlc_googlenet with cuDNN using batch_size:128 on a K40c: Average Forward pass: 562.841 ms. Average Backward pass: 1123.84 ms. Average Forward-Backward: 1688.8 ms. This model was trained by Sergio Guadarrama @sguada\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "283087f6a9c25d851e581ea19944ff9d",
        "graph_path": "bvlc_googlenet_1.0/frozen_model.pb"
      },
      "name": "BVLC_GoogLeNet_Caffe",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/BVLC/Caffe/tree/master/models/bvlc_googlenet",
        "https://arxiv.org/abs/1409.4842"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "PASCAL VOC 2012"
      },
      "description": "TensorFlow Semantic Segmentation model, which is trained on the COCO (Common Objects in Context) dataset. Use deeplabv3_mnv2_dm05_pascal_train_aug(deeplabv3_mnv2_dm05_pascal_train_aug_2018_10_01) from TensorFlow DeepLab Model Zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "0336ceb67b378df8ada0efe9eadb5ac8",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/deeplabv3_mnv2_dm05_pascal_train_aug_2018_10_01/frozen_inference_graph.pb"
      },
      "name": "DeepLabv3_MobileNet_v2_DM_05_PASCAL_VOC_Train_Aug",
      "output": {
        "description": "the output semantic segment",
        "type": "semanticsegment"
      },
      "reference": [
        "https://github.com/tensorflow/models/tree/master/research/deeplab",
        "https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "PASCAL VOC 2012"
      },
      "description": "TensorFlow Semantic Segmentation model, which is trained on the COCO (Common Objects in Context) dataset. Use deeplabv3_mnv2_dm05_pascal_trainval(deeplabv3_mnv2_dm05_pascal_trainval_2018_10_01) from TensorFlow DeepLab Model Zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "a2337e2840c4d4bbab005b519fc5c43e",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/deeplabv3_mnv2_dm05_pascal_trainval_2018_10_01/frozen_inference_graph.pb"
      },
      "name": "DeepLabv3_MobileNet_v2_DM_05_PASCAL_VOC_Train_Val",
      "output": {
        "description": "the output semantic segment",
        "type": "semanticsegment"
      },
      "reference": [
        "https://github.com/tensorflow/models/tree/master/research/deeplab",
        "https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "PASCAL VOC 2012"
      },
      "description": "TensorFlow Semantic Segmentation model, which is trained on the COCO (Common Objects in Context) dataset. Use mobilenetv2_coco_voc_trainaug(deeplabv3_mnv2_pascal_train_aug_2018_01_29) from TensorFlow DeepLab Model Zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "b0a1d0340189d7003291010abbc2e475",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29/frozen_inference_graph.pb"
      },
      "name": "DeepLabv3_MobileNet_v2_PASCAL_VOC_Train_Aug",
      "output": {
        "description": "the output semantic segment",
        "type": "semanticsegment"
      },
      "reference": [
        "https://github.com/tensorflow/models/tree/master/research/deeplab",
        "https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "PASCAL VOC 2012"
      },
      "description": "TensorFlow Semantic Segmentation model, which is trained on the COCO (Common Objects in Context) dataset. Use mobilenetv2_coco_voc_trainval(deeplabv3_mnv2_pascal_trainval_2018_01_29) from TensorFlow DeepLab Model Zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "bfc503739d93cedf973f82a5df1901eb",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/deeplabv3_mnv2_pascal_trainval_2018_01_29/frozen_inference_graph.pb"
      },
      "name": "DeepLabv3_MobileNet_v2_PASCAL_VOC_Train_Val",
      "output": {
        "description": "the output semantic segment",
        "type": "semanticsegment"
      },
      "reference": [
        "https://github.com/tensorflow/models/tree/master/research/deeplab",
        "https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "PASCAL VOC 2012"
      },
      "description": "TensorFlow Semantic Segmentation model, which is trained on the COCO (Common Objects in Context) dataset. Use deeplabv3_pascal_train_aug(deeplabv3_pascal_train_aug_2018_01_04) from TensorFlow DeepLab Model Zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "e6d2e7c8c9cf683e43ec052a5d8f62aa",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/deeplabv3_pascal_train_aug_2018_01_04/frozen_inference_graph.pb"
      },
      "name": "DeepLabv3_Xception_65_PASCAL_VOC_Train_Aug",
      "output": {
        "description": "the output semantic segment",
        "type": "semanticsegment"
      },
      "reference": [
        "https://github.com/tensorflow/models/tree/master/research/deeplab",
        "https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "PASCAL VOC 2012"
      },
      "description": "TensorFlow Semantic Segmentation model, which is trained on the COCO (Common Objects in Context) dataset. Use deeplabv3_pascal_trainval(deeplabv3_pascal_trainval_2018_01_04) from TensorFlow DeepLab Model Zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "ab5108a6fc6824c9cda9090a19181ca6",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/deeplabv3_pascal_trainval_2018_01_04/frozen_inference_graph.pb"
      },
      "name": "DeepLabv3_Xception_65_PASCAL_VOC_Train_Val",
      "output": {
        "description": "the output semantic segment",
        "type": "semanticsegment"
      },
      "reference": [
        "https://github.com/tensorflow/models/tree/master/research/deeplab",
        "https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "a7cba73fa2af9aa394658057adaf0f2d",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "Faster_RCNN_Inception_ResNet_v2_Atrous_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "558d79ebf9b67164f412c841690aba8d",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "Faster_RCNN_Inception_ResNet_v2_Atrous_Lowproposals_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use faster_rcnn_inception_v2_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "1f1902262c16c2d9acb9bc4f8a8c266f",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/faster_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "Faster_RCNN_Inception_v2_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use faster_rcnn_nas_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "3eb92f992d5781d61b1fa1067ce4b305",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/faster_rcnn_nas_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "Faster_RCNN_NAS_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use faster_rcnn_nas_lowproposals_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "32d670b40441a358d03a33d3b2d85d36",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/faster_rcnn_nas_lowproposals_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "Faster_RCNN_NAS_Lowproposals_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use faster_rcnn_resnet101_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "cbbd349318851ad33a92797db23c9972",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/faster_rcnn_resnet101_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "Faster_RCNN_ResNet101_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use faster_rcnn_resnet101_lowproposals_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "e8cc341b4dbde7cf5650fd6480989dc0",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/faster_rcnn_resnet101_lowproposals_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "Faster_RCNN_ResNet101_Lowproposals_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use faster_rcnn_resnet50_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "8b6edeb143566f830e35765438b452ff",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/faster_rcnn_resnet50_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "Faster_RCNN_ResNet50_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use faster_rcnn_resnet50_lowproposals_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "dc38b8692f73d6985a2c638b41ca2f16",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/faster_rcnn_resnet50_lowproposals_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "Faster_RCNN_Resnet50_Lowproposals_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "ImageNet"
      },
      "description": "This variant of the Inception model is easier to use for DeepDream and other imaging techniques. This is because it allows the input image to be any size, and the optimized images are also prettier. Inception 5h is equivalent to Inception V1.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "be71c0d3ba9c5952b11656133588c75c",
        "graph_path": "inception_5h.pb"
      },
      "name": "Inception_5h",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "http://arxiv.org/abs/1512.00567",
        "https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/inception5h.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "80.4",
        "top5": "95.3",
        "training_dataset": "ImageNet"
      },
      "description": "Inception-ResNet-v2, a convolutional neural network (CNN) that achieves a new state of the art in terms of accuracy on the ILSVRC image classification benchmark. Inception-ResNet-v2 is a variation of our earlier Inception V3 model which borrows some ideas from Microsoft's ResNet papers. The full details of the model are in our arXiv preprint Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "ee6ad0ddd389b325832d4c4113103118",
        "graph_path": "inception_resnet_v2_frozen.pb"
      },
      "name": "Inception_ResNet_v2",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "http://arxiv.org/abs/1512.00567",
        "https://github.com/tensorflow/models/tree/master/research/slim"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "69.8",
        "top5": "89.6",
        "training_dataset": "ImageNet"
      },
      "description": "Inception v1 architecture, implemented in the winning ILSVRC 2014 submission GoogLeNet. Intorduced in Going deeper with convolutions, Szegedy et al. (2014) This model is from TensorFlow Models Slim (inception_v1_2016_08_28.tar.gz).\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "da5144c55d2fb47cfd1b80e177230269",
        "graph_path": "inception_v1_frozen.pb"
      },
      "name": "Inception_v1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "http://arxiv.org/abs/1512.00567",
        "https://github.com/tensorflow/models/tree/master/research/slim"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "73.9",
        "top5": "91.8",
        "training_dataset": "ImageNet"
      },
      "description": "Inception-v2 introduced Factorization(factorize convolutions into smaller convolutions) and some minor change into Inception-v1. This model is from TensorFlow Models Slim (inception_v2_2016_08_28.tar.gz).\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "TODO",
      "model": {
        "graph_checksum": "f357c54648189679b5c1af1df348d8cf",
        "graph_path": "inception_v2_frozen.pb"
      },
      "name": "Inception_v2",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "http://arxiv.org/abs/1512.00567",
        "https://github.com/tensorflow/models/tree/master/research/slim"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "78.0",
        "top5": "93.9",
        "training_dataset": "ImageNet"
      },
      "description": "variant of Inception-v2 which adds BN-auxiliary. This model is from TensorFlow Models Slim (inception_v3_2016_08_28.tar.gz).\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "328f68f37ae0c180191e96c9b73a813e",
        "graph_path": "inception_v3_frozen.pb"
      },
      "name": "Inception_v3",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "http://arxiv.org/abs/1512.00567",
        "https://github.com/tensorflow/models/tree/master/research/slim"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "80.2",
        "top5": "95.2",
        "training_dataset": "ImageNet"
      },
      "description": "This model is from TensorFlow Models Slim (inception_v4_2016_09_09.tar.gz).\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "3322bc1ca7f9cc753e6c70c2177319ac",
        "graph_path": "inception_v4_frozen.pb"
      },
      "name": "Inception_v4",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "http://arxiv.org/abs/1512.00567",
        "https://github.com/tensorflow/models/tree/master/research/slim"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Instance Segmentation model, which is trained on the COCO (Common Objects in Context) dataset. Use mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "9dd24ee8716d34789efbf6b50c8f2ee1",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "Mask_RCNN_Inception_ResNet_v2_Atrous_COCO",
      "output": {
        "description": "the output instance segment",
        "type": "instancesegment"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Instance Segmentation model, which is trained on the COCO (Common Objects in Context) dataset. Use mask_rcnn_inception_v2_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "b47e443b313a709e4c39c1caeaa3ecb3",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/mask_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "Mask_RCNN_Inception_v2_COCO",
      "output": {
        "description": "the output instance segment",
        "type": "instancesegment"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Instance Segmentation model, which is trained on the COCO (Common Objects in Context) dataset. Use mask_rcnn_resnet101_atrous_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "0800f8fc9c40fd8a9caf89d608df5ae9",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/mask_rcnn_resnet101_atrous_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "Mask_RCNN_ResNet101_v2_Atrous_COCO",
      "output": {
        "description": "the output instance segment",
        "type": "instancesegment"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Instance Segmentation model, which is trained on the COCO (Common Objects in Context) dataset. Use mask_rcnn_resnet50_atrous_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "09d9045ce3b1eef50466b129c95543e8",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/mask_rcnn_resnet50_atrous_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "Mask_RCNN_ResNet50_v2_Atrous_COCO",
      "output": {
        "description": "the output instance segment",
        "type": "instancesegment"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "71.23",
        "training_dataset": "ImageNet"
      },
      "description": "MLPerf_Mobilenet_v1_1.0_224. Use mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224.tgz from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "d5f69cef81ad8afb335d9727a17c462a",
        "graph_path": "mobilenet_v1_1.0_224_frozen.pb",
        "is_archive": true
      },
      "name": "MLPerf_Mobilenet_v1",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/mlperf/inference/tree/master/v0.5/classification_and_detection",
        "https://github.com/mlperf/training/tree/master/image_classification",
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "76.16",
        "training_dataset": "ImageNet"
      },
      "description": "MLPerf_ResNet50_v1.5.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "7b94a2da05dd30f6c0af23a46bc08886",
        "graph_path": "resnet50_v1.pb"
      },
      "name": "MLPerf_ResNet50_v1.5",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1512.03385.pdf",
        "https://github.com/mlperf/inference/tree/master/v0.5/classification_and_detection",
        "https://github.com/mlperf/training/tree/master/image_classification"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "mAP": "0.22",
        "manifest_author": "Cheng Li",
        "training_dataset": "COCO"
      },
      "description": "MLPerf_SSD_MobileNet_300x300. Use ssd_mobilenet_v1_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "c9df908a779062ce00abe8f98e4a3eb1",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/ssd_mobilenet_v1_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "MLPerf_SSD_MobileNet_v1_300x300",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/mlperf/inference/tree/master/v0.5/classification_and_detection",
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "mAP": "0.20",
        "manifest_author": "Cheng Li",
        "training_dataset": "COCO"
      },
      "description": "MLPerf_SSD_ResNet34_1200x1200.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "2831b0a188efbb32c5e3e0c6cb6cc770",
        "graph_path": "https://zenodo.org/record/3262269/files/ssd_resnet34_mAP_20.2.pb"
      },
      "name": "MLPerf_SSD_ResNet34_1200x1200",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/mlperf/inference/tree/master/v0.5/classification_and_detection",
        "https://github.com/mlperf/inference/tree/master/others/cloud/single_stage_detector/tensorflow"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "41.5",
        "top5": "66.3",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "0fdca2e0ef5a760143da81bca3bb504f",
        "graph_path": "mobilenet_v1_0.25_128_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.25_128",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_128.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "39.5",
        "top5": "64.4",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "f2f15e647262bb62f80a870b82f6c490",
        "graph_path": "mobilenet_v1_0.25_128_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.25_128_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_128_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "45.5",
        "top5": "70.3",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "156f268e686210a122dd457c82399d63",
        "graph_path": "mobilenet_v1_0.25_160_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.25_160",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_160.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "43.4",
        "top5": "68.5",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "b14242d4c7b83b60bb1564adf47d5572",
        "graph_path": "mobilenet_v1_0.25_160_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.25_160_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_160_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "47.7",
        "top5": "72.3",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "ffea6fa6911f807537239e4b0a784609",
        "graph_path": "mobilenet_v1_0.25_192_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.25_192",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_192.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "46.",
        "top5": "71.2",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "5f92d6bf03f102a0c8e5bc4c21f6ff6d",
        "graph_path": "mobilenet_v1_0.25_192_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.25_192_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_192_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "49.8",
        "top5": "74.2",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "865822137db139d1cc27570f7977e84c",
        "graph_path": "mobilenet_v1_0.25_224_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.25_224",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_224.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "48.",
        "top5": "72.8",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "374974e4565ae11ffdfc593e2367ccbd",
        "graph_path": "mobilenet_v1_0.25_224_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.25_224_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_224_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "56.3",
        "top5": "79.4",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "882a6d44a4c860fbf4cdaff45230983d",
        "graph_path": "mobilenet_v1_0.5_128_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.5_128",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.5_128.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "54.5",
        "top5": "77.7",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "def62267e4e781b0ad18613951580b2f",
        "graph_path": "mobilenet_v1_0.5_128_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.5_128_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.5_128_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "59.1",
        "top5": "81.9",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "f75fba66ae24dbb907dc4ea26cfb0674",
        "graph_path": "mobilenet_v1_0.5_160_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.5_160",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.5_160.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "57.7",
        "top5": "80.4",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "80890892120d884bb6283ba145f0a210",
        "graph_path": "mobilenet_v1_0.5_160_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.5_160_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.5_160_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "61.7",
        "top5": "83.6",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "fe41acaa04487cc35e2bf59a7cd8e5e4",
        "graph_path": "mobilenet_v1_0.5_192_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.5_192",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.5_192.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "60.",
        "top5": "82.2",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "f93309968626f91a8a2d5d19551313bb",
        "graph_path": "mobilenet_v1_0.5_192_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.5_192_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.5_192_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "63.3",
        "top5": "84.9",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "5f4f22e48c630d547401d988f3821f7d",
        "graph_path": "mobilenet_v1_0.5_224_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.5_224",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.5_224.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "60.7",
        "top5": "83.2",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "e2426f52bdb09acacf3b5aa0a0340731",
        "graph_path": "mobilenet_v1_0.5_224_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.5_224_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.5_224_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "62.1",
        "top5": "83.9",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "dcd24ae3690aaf27f1acfbe6d10ec1a3",
        "graph_path": "mobilenet_v1_0.75_128_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.75_128",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.75_128.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "55.8",
        "top5": "78.8",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "1872803beb64c5813b458e45bce1d52f",
        "graph_path": "mobilenet_v1_0.75_128_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.75_128_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.75_128_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "65.3",
        "top5": "86.",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "80bb07b2e1fb30f58bd7e5fec802adae",
        "graph_path": "mobilenet_v1_0.75_160_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.75_160",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.75_160.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "f9b875858c908b2315f40f821645e67f",
        "graph_path": "mobilenet_v1_0.75_160_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.75_160_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.75_160_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "67.2",
        "top5": "87.3",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "5dd55f5254cb77f4e42835a591de7d8c",
        "graph_path": "mobilenet_v1_0.75_192_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.75_192",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.75_192.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "66.1",
        "top5": "86.4",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "3d63ab2bf42dbf5fdf9624733c2ca3c8",
        "graph_path": "mobilenet_v1_0.75_192_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.75_192_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.75_192_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "68.4",
        "top5": "88.2",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "d8535f9da0fd671d1cd1b86888ce6d11",
        "graph_path": "mobilenet_v1_0.75_224_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.75_224",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.75_224.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "66.8",
        "top5": "87.",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "bf0d7b02c9f2c70620065b814a76442f",
        "graph_path": "mobilenet_v1_0.75_224_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_0.75_224_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.75_224_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "65.2",
        "top5": "85.8",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "e0e2d7f23f2bc75f93d10a049c7a86c4",
        "graph_path": "mobilenet_v1_1.0_128_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_1.0_128",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_128.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "63.4",
        "top5": "84.2",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "81ae6bd74c74cf7096f7c9adf833d986",
        "graph_path": "mobilenet_v1_1.0_128_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_1.0_128_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_128_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "68.",
        "top5": "87.7",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "0733327af76a83409c974979873bc9c7",
        "graph_path": "mobilenet_v1_1.0_160_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_1.0_160",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_160.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "fcf4cb5916bc2d5f7849bc44206ce69d",
        "graph_path": "mobilenet_v1_1.0_160_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_1.0_160_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_160_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "70.",
        "top5": "89.2",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "1054749ae43d8abeaf2091dddfee7b04",
        "graph_path": "mobilenet_v1_1.0_192_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_1.0_192",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_192.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "69.2",
        "top5": "88.3",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "85b291b2662a1fb049b41478f46914a4",
        "graph_path": "mobilenet_v1_1.0_192_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_1.0_192_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_192_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "70.9",
        "top5": "89.9",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "7f564c782f0141e4e7bdc5161a6cc2f4",
        "graph_path": "mobilenet_v1_1.0_224_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_1.0_224",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "top1": "70.1",
        "top5": "88.9",
        "training_dataset": "ImageNet"
      },
      "description": "MobileNet is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices. This network consists of 224 layers and achieves 70.9 top-1 and 89.9 top-5 accuracy.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "fb464017c6275ee283dc718fe7b83e17",
        "graph_path": "mobilenet_v1_1.0_224_quant_frozen.pb",
        "is_archive": true
      },
      "name": "MobileNet_v1_1.0_224_Quant",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md",
        "http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224_quant.tgz",
        "https://arxiv.org/pdf/1704.04861.pdf",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "76.4",
        "top5": "92.9",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification network built of layers that learn residual functions w.r.t layer inputs. This model is from TensorFlow Models Slim (resnet_v1_101_2016_08_28.tar.gz)\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "939ed8e447ef607fea631c568fd1dd05",
        "graph_path": "resnet_v1_101_frozen.pb"
      },
      "name": "ResNet_v1_101",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1512.03385.pdf",
        "https://github.com/tensorflow/models/tree/master/research/slim"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "76.8",
        "top5": "93.2",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification network built of layers that learn residual functions w.r.t layer inputs. This model is from TensorFlow Models Slim (resnet_v1_152_2016_08_28.tar.gz)\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "cf460292f3c82b73e35a0fc9ebaa69c4",
        "graph_path": "resnet_v1_152_frozen.pb"
      },
      "name": "ResNet_v1_152",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1512.03385.pdf",
        "https://github.com/tensorflow/models/tree/master/research/slim"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "75.2",
        "top5": "92.2",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification network built of layers that learn residual functions w.r.t layer inputs. This network consists of 50 layers and achieves 75.2 top-1 and 92.2 top-5 accuracy on ILSVRC-2015. This model is from TensorFlow Models Slim (resnet_v1_50_2016_08_28.tar.gz).\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "b153619627b5811a3b96c255126dd507",
        "graph_path": "resnet_v1_50_frozen.pb"
      },
      "name": "ResNet_v1_50",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1512.03385.pdf",
        "https://github.com/tensorflow/models/tree/master/research/slim"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "77.0",
        "top5": "93.7",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification network built of layers that learn residual functions w.r.t layer inputs. This model is from TensorFlow Models Slim (resnet_v2_101_2017_04_14.tar.gz).\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "5c3490640ea43a1cf93fe908b7e32947",
        "graph_path": "resnet_v2_101_frozen.pb"
      },
      "name": "ResNet_v2_101",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1512.03385.pdf",
        "https://github.com/tensorflow/models/tree/master/research/slim"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "77.8",
        "top5": "94.1",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification network built of layers that learn residual functions w.r.t layer inputs. This model is from TensorFlow Models Slim (resnet_v2_152_2017_04_14.tar.gz).\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "bb160c9e548566ebb1225396893963be",
        "graph_path": "resnet_v2_152_frozen.pb"
      },
      "name": "ResNet_v2_152",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1512.03385.pdf",
        "https://github.com/tensorflow/models/tree/master/research/slim"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "75.6",
        "top5": "92.8",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification network built of layers that learn residual functions w.r.t layer inputs. This model is from TensorFlow Models Slim (resnet_v2_50_2017_04_14.tar.gz).\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "b7222f53a052c9a51aa6c6b96249d171",
        "graph_path": "resnet_v2_50_frozen.pb"
      },
      "name": "ResNet_v2_50",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1512.03385.pdf",
        "https://github.com/tensorflow/models/tree/master/research/slim"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use rfcn_resnet101_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "5538b70ce09cedf3c656157e4a3b06f1",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/rfcn_resnet101_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "RFCN_ResNet101_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "DIV2K - bicubic downscaling x4 competition"
      },
      "description": "TensorFlow Image Enhancement model, which is trained on DIV2K - bicubic downscaling x4 competition. Use SRGAN release 1.2.0 from TensorLayer SRGAN repo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "4af37a6975db591bfd1c780eb8019f97",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/srgan_1.2/frozen_model.pb"
      },
      "name": "SRGAN",
      "output": {
        "description": "the output image",
        "type": "image"
      },
      "reference": [
        "https://github.com/tensorlayer/srgan",
        "https://github.com/tensorlayer/srgan/releases/download/1.2.0/g_srgan.npz"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use ssd_inception_v2_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "22632401e2114b3a0fc79db7b02bbc7c",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/ssd_inception_v2_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "SSD_Inception_v2_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "182839f117af757c4208f28ad25e3749",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03/frozen_inference_graph.pb"
      },
      "name": "SSD_MobileNet_v1_0.75_Depth_300x300_COCO14_Sync",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "mAP": "0.22",
        "manifest_author": "Cheng Li",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use ssd_mobilenet_v1_coco_2018_01_28 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "c9df908a779062ce00abe8f98e4a3eb1",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/ssd_mobilenet_v1_coco_2018_01_28/frozen_inference_graph.pb"
      },
      "name": "SSD_MobileNet_v1_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "bd60c1d92f4f7985fce54ecc77768ef1",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/frozen_inference_graph.pb"
      },
      "name": "SSD_MobileNet_v1_FPN_Shared_Box_Predictor_640x640_COCO14_Sync",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync_2018_07_03 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "157acb6f72f214655252d59d5040be54",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync_2018_07_03/frozen_inference_graph.pb"
      },
      "name": "SSD_MobileNet_v1_PPN_Shared_Box_Predictor_300x300_COCO14_Sync",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use ssd_mobilenet_v2_coco_2018_03_29 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "310fbd22691b984c709f2dbf6553f58e",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb"
      },
      "name": "SSD_MobileNet_v2_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "753c589fa596c855e42cf82eb463c637",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/frozen_inference_graph.pb"
      },
      "name": "SSD_ResNet50_FPN_Shared_Box_Predictor_640x640_COCO14_Sync",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Jingning Tang",
        "training_dataset": "COCO"
      },
      "description": "TensorFlow Object Detection model, which is trained on the COCO (Common Objects in Context) dataset. Use ssdlite_mobilenet_v2_coco_2018_05_09 from TensorFlow detection model zoo.\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "Apache License, Version 2.0",
      "model": {
        "graph_checksum": "87a63b4b0b7cb492bff0bead10af7fae",
        "graph_path": "https://s3.amazonaws.com/store.carml.org/models/tensorflow/models/ssdlite_mobilenet_v2_coco_2018_05_09/frozen_inference_graph.pb"
      },
      "name": "SSDLite_MobileNet_v2_COCO",
      "output": {
        "description": "the output bounding box",
        "type": "boundingbox"
      },
      "reference": [
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "71.5",
        "top5": "89.8",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification convolutional network. VGG19 uses a network with 19 layers of 3x3 convolution filters. VGG19 achieves 71.1% top-1 and 89.8% top-5 accuracy on the ImageNet challenge dataset in 2015. This model is from TensorFlow Models Slim (vgg_16_2016_08_28.tar.gz).\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "b44e96ab732a819710c74ec2462659ac",
        "graph_path": "vgg_16_frozen.pb"
      },
      "name": "VGG16",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1409.1556.pdf",
        "https://github.com/tensorflow/models/tree/master/research/slim"
      ],
      "version": "1.0"
    },
    {
      "attributes": {
        "kind": "CNN",
        "manifest_author": "Cheng Li",
        "top1": "71.1",
        "top5": "89.8",
        "training_dataset": "ImageNet"
      },
      "description": "An image-classification convolutional network. VGG19 uses a network with 19 layers of 3x3 convolution filters. VGG19 achieves 71.1% top-1 and 89.8% top-5 accuracy on the ImageNet challenge dataset in 2015. This model is from TensorFlow Models Slim (vgg_16_2016_08_28.tar.gz).\n",
      "framework": {
        "name": "TensorFlow",
        "version": "1.14.0"
      },
      "inputs": [
        {
          "description": "the input image",
          "type": "image"
        }
      ],
      "license": "unrestricted",
      "model": {
        "graph_checksum": "9b09d4bdc8fcee61678ec44107d92190",
        "graph_path": "vgg_19_frozen.pb"
      },
      "name": "VGG19",
      "output": {
        "description": "the output label",
        "type": "classification"
      },
      "reference": [
        "https://arxiv.org/pdf/1409.1556.pdf",
        "https://github.com/tensorflow/models/tree/master/research/slim"
      ],
      "version": "1.0"
    }
  ]
}
